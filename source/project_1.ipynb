{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "# importing the module\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#additional functions:\n",
    "from add_functions.proj1_helpers import *\n",
    "from add_functions.ml_methods_labs import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: \n",
    "\n",
    "- train.csv - Training set of 250000 events. The file starts with the ID column, then the label column (the y you have to predict), and finally 30 feature columns.\n",
    "- test.csv - The test set of around 568238 events - Everything as above, except the label is missing.\n",
    "- sample-submission.csv - a sample submission file in the correct format. The sample submission always predicts -1, that is ‘background’.\n",
    "\n",
    "Note: \n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training data shape: y {y.shape}, X {tX[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Invalid data cleaning\n",
    "\n",
    "### First, we should remove all the -999 values and replace it by something usefull. \n",
    "### We have two choices. Either we remove the entire sample that contains -999\n",
    "### Or we replace this value with the mean of all the other values for the given feature\n",
    "\n",
    "def replace_invalid_values_with_mean(tX, invalid_identifier):\n",
    "    \"\"\"Replaces invalid values with the mean of all the values in the cooresponding feature \"\"\"\n",
    "    nb_features = tX.shape[1]\n",
    "    \n",
    "    for i in range(nb_features):\n",
    "        input_data = tX[:,i] # list of the given feature for all the samples\n",
    "        input_data_copy = input_data.copy()\n",
    "        np.delete(input_data_copy, np.where(input_data_copy == invalid_identifier)) # deleting all occurences of invalid_identifier\n",
    "        valid_data_mean_1 = np.mean(input_data_copy) # mean of input data without invalid values\n",
    "        tX[tX[:,i]==invalid_identifier,i] = valid_data_mean_1 # replacing all the invalid data with the above mean\n",
    "\n",
    "    return tX\n",
    "\n",
    "invalid_identifier = -999\n",
    "tX = replace_invalid_values_with_mean(tX, invalid_identifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outlayers removing\n",
    "\n",
    "def replace_outlayers_values_with_mean(tX, delta):\n",
    "    \"\"\"Replaces outlayers values with the mean of all the values in the cooresponding feature \"\"\"\n",
    "    mean = tX.mean(axis=0).reshape(1, -1)\n",
    "    std_deviation = tX.std(axis=0).reshape(1, -1)\n",
    "    \n",
    "    threshold = delta*std_deviation\n",
    "    result = np.where(np.abs(tX - mean) < threshold*np.ones_like(tX), tX, mean)\n",
    "    return result\n",
    "\n",
    "delta = 3 # we consider a value as outlay when x > delta*std_deviationtX = replace_outlayers_values_with_mean(tX, delta)\n",
    "tX = replace_outlayers_values_with_mean(tX, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tX[:,9], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding of y: \n",
    "y_enc = (y+1)/2\n",
    "y = y_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ML methods:\n",
    "\n",
    "You should take care of the following:\n",
    "- Return type: Note that all functions should return: (w, loss), which is the last weight vector of the method, and the corresponding loss value (cost function). Note that while in previous labs you might have kept track of all encountered w for iterative methods, here we only want the last one.\n",
    "- File names: Please provide all function implementations in a single python file, called implementations.py.\n",
    "- All code should be easily readable and commented.\n",
    "- Note that we might automatically call your provided methods and evaluate for correct implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation:\n",
    "Create cross validation method in order to evaluate the loss better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed=1):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_sets(tX, y, k_indices,i):\n",
    "    \"\"\"\n",
    "    cross_validation_sets: separates tX and y randomly into training and validation sets. \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - int K: number of folds during k-fold cross-val\n",
    "    @output: \n",
    "    - np.array(percentage*N,m) tX_train: training features\n",
    "    - np.array(percentage*N,) y_train: training labels\n",
    "    - np.array((1-percentage)*N,m) tX_val: validation features\n",
    "    - np.array((1-percentage)**N,) y_val: validation labels\n",
    "    \"\"\"\n",
    "    \n",
    "    train_indices = np.concatenate(np.delete(k_indices, i, axis = 0), axis = 0)\n",
    "    val_indices = k_indices[i]\n",
    "    \n",
    "    #creates training and validation:\n",
    "    tX_train = np.take(tX, train_indices, axis = 0)\n",
    "    y_train = np.take(y, train_indices, axis = 0)\n",
    "    tX_val = np.take(tX, val_indices, axis = 0)\n",
    "    y_val = np.take(y, val_indices, axis = 0)\n",
    "    \n",
    "    size = len(train_indices) + len(val_indices)\n",
    "    \n",
    "    assert (tX_train.shape[0] + tX_val.shape[0] == size)\n",
    "    assert (y_train.shape[0] + y_val.shape[0] == size)\n",
    "\n",
    "    return tX_train, y_train, tX_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models that will be trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LS_GD': 'Least squares with GD',\n",
    "    'LS_SGD': 'Least squares with SGD',\n",
    "    'LS_normal': 'Least squares with normal equations',\n",
    "    'RR_normal': 'Ridge regression with normal equations',\n",
    "    'LR': 'Logistic regression with GD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default json parameters for training, for special parameters take other file or create your own dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Dimension reduction:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tX_std = standardize(tX)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(tX_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "#tX_pc = pd.DataFrame(pca.fit_transform(tX_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc = pd.DataFrame(pca.fit_transform(tX_std))\n",
    "#tX_pc['label'] = y\n",
    "#tX_pc['index'] = tX_pc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc['label'] = y\n",
    "tX_pc['index'] = tX_pc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc_sample = tX_pc.iloc[np.random.choice(len(tX_std),200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tX_pc_sample[0],tX_pc_sample[1], c=tX_pc_sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#sns.pairplot(tX_pc_sample, hue = 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tX_pc[0],tX_pc[1], c=tX_pc['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize = (10, 5))\n",
    "sns.distplot(tX_pc[0], hist=True, kde=True, ax = axs[0])\n",
    "axs[0].set_title('First comp')\n",
    "sns.distplot(tX_pc[1], hist=True, kde=True, ax = axs[1])\n",
    "axs[1].set_title('Second comp')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter outliers: \n",
    "tX_pc = tX_pc[tX_pc[0]<10]\n",
    "tX_pc = tX_pc[tX_pc[1]<10]\n",
    "plt.scatter(tX_pc[0],tX_pc[1], c=tX_pc['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of points to keep:\n",
    "indices = tX_pc.index\n",
    "tX_std_filtered = pd.DataFrame(tX_std).iloc[indices].values\n",
    "y_filtered = pd.DataFrame(y).iloc[indices].values.T[0]\n",
    "print('Remove {x} outliers'.format(\n",
    "    x=len(set(range(len(tX_std))).difference(set(indices)))))\n",
    "print('New shape of tX_std: {s} and y: ({y},)'.format(s=tX_std_filtered.shape,\n",
    "                                                    y=len(y_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_std = tX_std_filtered\n",
    "y = y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k_indices = build_k_indices(tX, K)\n",
    "tX_train, y_train, tX_val, y_val = cross_validation_sets(tX, y, k_indices, i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion:\n",
    "Polynomial feature expansion: $X' = [1, X, X^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feats(tX, degree):  \n",
    "    if not np.array_equal(tX[:, 0], np.ones(len(tX))):\n",
    "            # add bias term:\n",
    "            tX_poly = np.hstack((np.ones((len(tX), 1)), tX))\n",
    "    for deg in range(degree, degree+1):\n",
    "        tX_poly = np.c_[tX_poly, np.power(tX, deg)]\n",
    "    return tX_poly\n",
    "\"\"\"\n",
    "Test:\n",
    "assert (np.array_equal(\n",
    "    pd.DataFrame(build_poly(tX, 2)).values[:, 1:31],\n",
    "    pd.DataFrame(tX).values[:, 0:30]))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: \n",
    "Global training function where each model can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(tX,\n",
    "                y,\n",
    "                model='LS_GD',\n",
    "                initial_w=None,\n",
    "                param=default_parameters,\n",
    "                verbose=True\n",
    "                ):\n",
    "    \"\"\"\n",
    "    train_model: trains a model according to their parameters in the dictionnary\n",
    "    @input:\n",
    "        - np.array(N,) y: labels\n",
    "        - np.array(N,m) tX: features\n",
    "        - string model: 'LS_GD', 'LS_SGD', 'LS_normal', 'RR_normal' or 'LR'\n",
    "        - np.array(m,) initial_w: starting weights for GD and SGD\n",
    "        - dictionary param: different parameters required for training (lr, lambda, etc)\n",
    "    @output: \n",
    "        - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "        - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    # will keep last weights and last loss\n",
    "    weights, loss = [], []\n",
    "    parameters = param[model]\n",
    "\n",
    "    # parameters:\n",
    "    K, MAX_ITERS, POLY = parameters['K'], parameters['max_iters'], parameters[\n",
    "        'poly']\n",
    "\n",
    "    # will keep all losses during training\n",
    "    costs_ = np.zeros([K, MAX_ITERS])\n",
    "    train_costs_ = np.zeros([K, MAX_ITERS])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\n",
    "            f'{models[model]} starting with: \\nInitial_w: {initial_w},\\nParameters:{parameters}'\n",
    "        )\n",
    "        print('------------------')\n",
    "        print('START TRAINING:')\n",
    "\n",
    "    # get indices of k-fold:\n",
    "    k_indices = build_k_indices(y, K)\n",
    "\n",
    "    # polynonmial expansion:\n",
    "    tX_pol = poly_feats(tX, POLY)\n",
    "\n",
    "    # standardize:\n",
    "    tX_std = standardize(tX_pol)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Data shape:{tX_std.shape}')\n",
    "\n",
    "    # initial_w:\n",
    "    if initial_w == None:\n",
    "        initial_w = np.zeros(tX_std.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        if verbose:\n",
    "            print(f'K = {i+1}')\n",
    "        # create training and validation sets:\n",
    "        tX_train, y_train, tX_val, y_val = cross_validation_sets(\n",
    "            tX_std, y, k_indices, i)\n",
    "\n",
    "        #start with initial_w:\n",
    "        w = initial_w.copy()\n",
    "        # keep tabs on losses during training\n",
    "        cost_history = []\n",
    "        train_cost_history = []\n",
    "\n",
    "        # if RR_normal or LS_normal no GD:\n",
    "        if model == 'RR_normal':\n",
    "            w, cost_history = ridge_regression_update(tX_train, y_train,\n",
    "                                                      tX_val, y_val,\n",
    "                                                      parameters)\n",
    "            cost = cost_history\n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "        elif model == 'LS_normal':\n",
    "            w, cost_history = least_squares_update(tX_train, y_train, tX_val,\n",
    "                                                   y_val)\n",
    "            cost = cost_history\n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "        \n",
    "        # else GD:\n",
    "        else:\n",
    "            for j in range(MAX_ITERS):\n",
    "                if model == 'LR':\n",
    "                    # calculate loss:\n",
    "                    cost = cost_logistic(tX_val, y_val, w, parameters)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(\n",
    "                        cost_logistic(tX_train, y_train, w, parameters))\n",
    "                    # Update weights:\n",
    "                    w = update_weights_logistic(tX_train, y_train, w,\n",
    "                                                parameters)\n",
    "                if model == 'LS_GD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(y_val, tX_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(MSE_loss(y_train, tX_train, w))\n",
    "                    # Update weights:\n",
    "                    w = update_weights_LS_GD(tX_train, y_train, w, parameters)\n",
    "                if model == 'LS_SGD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(y_val, tX_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(MSE_loss(y_train, tX_train, w))\n",
    "                    # update weights:\n",
    "                    w = update_weights_LS_SGD(tX_train, y_train, w, parameters)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "        # Add last weights and loss for auditing:\n",
    "        weights.append(w)\n",
    "        loss.append(cost)\n",
    "        costs_[i] = cost_history\n",
    "        if model != 'LS_normal' and model != 'RR_normal':\n",
    "            train_costs_[i] = train_cost_history\n",
    "        if verbose:\n",
    "            print('------------------')\n",
    "    \n",
    "    if verbose:\n",
    "        print('Average loss: {avg_loss}'.format(avg_loss=np.mean(loss)))\n",
    "\n",
    "    # best weights of min loss:\n",
    "    best_w = weights[np.argmin(loss)]\n",
    "\n",
    "    # Plot loss evolution for GD:\n",
    "    if model != 'LS_normal' and model != 'RR_normal' and verbose:\n",
    "        print('------------------')\n",
    "        print('Loss evolution:')\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].plot(costs_.T)\n",
    "        ax[0].set_title(models[model] + ' , validation loss')\n",
    "        ax[0].legend(range(1, K + 1))\n",
    "\n",
    "        ax[1].plot(train_costs_.T)\n",
    "        ax[1].set_title(models[model] + ' , training loss')\n",
    "        ax[1].legend(range(1, K + 1))\n",
    "\n",
    "    return best_w, np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares GD: \n",
    "Linear regression using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_MSE(y, tx, w):\n",
    "    \"\"\"\n",
    "    gradient_MSE: calculates the gradient of the MSE function \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: np.array(m,) gradient of MSE\n",
    "    \"\"\"\n",
    "    return (-1 / len(y)) * tx.T @ (y - tx @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE_loss(y, tx, w):\n",
    "    \"\"\"\n",
    "    MSE_loss: calculates the MSE loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: double, MSE loss\n",
    "    \"\"\"\n",
    "    assert(w.shape[0] == tx.shape[1])\n",
    "    assert(y.shape[0] == tx.shape[0])\n",
    "    MSE = np.square(np.subtract(y,tx @ w)).mean()\n",
    "    return MSE\n",
    "\n",
    "# Test MSE loss function with random weight vector:\n",
    "\"\"\"from sklearn.metrics import mean_squared_error\n",
    "w = np.random.rand(tX.shape[1]+1)\n",
    "assert (mean_squared_error(y, tX_std @ w) == MSE_loss(y, tX_std, w))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_GD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_GD: one step of GD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to GD\n",
    "    \"\"\"    \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # calculate gradient:\n",
    "    grad = gradient_MSE(y, tX, w)\n",
    "    # update weights:\n",
    "    w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(tx, y, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_GD: training with least squares GD\n",
    "    @input:\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    model = 'LS_GD'\n",
    "    best_w, avg_loss = train_model(tX=tx,\n",
    "                                   y=y,\n",
    "                                   model=model,\n",
    "                                   initial_w=initial_w,\n",
    "                                   param=parameters)\n",
    "\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# polynomial feature expansion:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "\n",
    "best_w, avg_loss = least_squares_GD(tx=tX,\n",
    "                                    y=y,\n",
    "                                    initial_w=None,\n",
    "                                    parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares SGD:\n",
    "Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_SGD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_SGD: one step of SGD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to SGD\n",
    "    \"\"\"   \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # batch_size to 1 for SGD, otherwise mini-batch:\n",
    "    batch_size = parameters['batch_size']\n",
    "\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tX, batch_size):\n",
    "        # calculate gradient on batch:\n",
    "        grad = gradient_MSE(minibatch_y, minibatch_tx, w)\n",
    "        # update weights:\n",
    "        w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(tx, y,initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_SGD: training with least squares SGD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LS_SGD'\n",
    "    best_w, avg_loss = train_model(tX = tx, y= y, \n",
    "                                   model=model, initial_w=initial_w, \n",
    "                                   param= parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS-SGD:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = least_squares_SGD(tx=tX,\n",
    "                                    y=y,\n",
    "                                    initial_w=None,\n",
    "                                    parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares:\n",
    "Least squares regression using normal equations. Solve $A^TAx = A^Ty$, Ax = b → x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_update(tX_train, y_train, tX_val, y_val):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    @output: \n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"        \n",
    "    # \"train\": \n",
    "    w = np.linalg.solve(tX_train.T @ tX_train, tX_train.T @ y_train)\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(y_val, tX_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas nécessaire de garder cette fonction `least_squares` si c'est ok d'avoir juste une fx train, un peu inutile... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx, parameters):\n",
    "    \"\"\"\n",
    "    least_squares: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LS_normal'\n",
    "    best_w, avg_loss = train_model(tX = tx, y= y, \n",
    "                                   model=model,\n",
    "                                   param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS_normal:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = least_squares(y = y, tx = tX, parameters = default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression: \n",
    "Ridge regression using normal equations. $w^* = (X^T X+\\lambda I)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_update(tX_train, y_train, tX_val, y_val, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output:\n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"\n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "\n",
    "    # \"train\":\n",
    "    w = np.linalg.solve(\n",
    "        tX_train.T @ tX_train + lambda_ * np.eye(tX_train.shape[1]),\n",
    "        tX_train.T @ y_train)\n",
    "\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(y_val, tX_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nouveau, pas nécessaire de garder cette fonction `ridge_regression` si c'est ok d'avoir juste une fx train, un peu inutile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, parameters):\n",
    "    \"\"\"\n",
    "    ridge_regression: weights with normal equations of ridge regression\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'RR_normal'\n",
    "    best_w, avg_loss = train_model(tX = tx, y= y, \n",
    "                                   model=model,\n",
    "                                   param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of RR_normal:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = ridge_regression(y = y, tx = tX, parameters = default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (regularized) regression: \n",
    "Logistic regression using gradient descent or SGD. \n",
    "https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb\n",
    "\n",
    "https://machinelearningmedium.com/2017/09/15/regularized-logistic-regression/\n",
    "\n",
    "- cost : $J(w) = -1/m (-y^T log (h)-(1-y)^T log (1-h)) + \\lambda/(2m)||w||^2 ; h = sigmoid(Xw)$\n",
    "\n",
    "Regularized logistic regression using gradient descent or SGD. $ w^* = min_w \\sum_{i=1}^n log(1+exp(-y_i w^T x_i)) +\\lambda ‖w‖^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    \"\"\"\n",
    "    sigmoid_activation: calculates the sigmoid activation of a vector z\n",
    "    @output: np.array(m,) \n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    cost_logistic: calculates the logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - double cost: logistic loss\n",
    "    \"\"\"     \n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "    \n",
    "    predictions = sigmoid_activation(tX @ w)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #Take the sum of both costs: error when label=1 + error when label=0\n",
    "    cost = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    \n",
    "    #Take the average cost:\n",
    "    cost = cost.sum() / m\n",
    "\n",
    "    # regularizer:\n",
    "    if lambda_:\n",
    "        cost += (lambda_ / (2 * m)) * w.T @ w\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_logistic: one iteration with GD on logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights to be updated\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) w: new weights\n",
    "    \"\"\"     \n",
    "    \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    lambda_ = parameters['lambda_']\n",
    "    m = len(tX)\n",
    "    \n",
    "    #Get Predictions:\n",
    "    predictions = sigmoid_activation(np.dot(tX, w))\n",
    "\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = (tX.T @ (predictions - y))\n",
    "\n",
    "    # if regularization\n",
    "    if lambda_:\n",
    "        gradient += lambda_ * w\n",
    "    \n",
    "    gradient /= m\n",
    "    \n",
    "    # Subtract from our weights to minimize cost\n",
    "    w -= lr * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    logistic_regression: logistic (regularized) regression with GD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LR'\n",
    "    best_w, avg_loss = train_model(tX = tx, y= y, \n",
    "                                   model=model, initial_w=initial_w, \n",
    "                                   param= parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of logistic regularized regression:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = logistic_regression(tx=tX,\n",
    "                                    y=y,initial_w=None,\n",
    "                                    parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Grid search\n",
    "[Cross-Validation and Hyperparameter Tuning: How to Optimise your Machine Learning Model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def randomized_grid_search(model, tX, y, initial_w, parameters_lists, nb_rnd_test, verbose=False):\n",
    "    '''\n",
    "    randomized_grid_search: Search the best parameters with the model among\n",
    "    a list of value for each parameter.\n",
    "    @input:\n",
    "    - string model: 'LS_GD', 'LS_SGD', 'LS_normal', 'RR_normal' or 'LR'\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(m,) initial_w: starting weights for GD and SGD\n",
    "    - dictionary parameters_lists: A dictionary of lists of values for the parameters\n",
    "        required for training (lr, lambda, etc)\n",
    "    - int nb_rnd_test: number of random test to do before giving the best results it found.\n",
    "    @output\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - dictionary new_params: The parameters we acheived the best result with\n",
    "    - int nb_rnd_test: same as the input\n",
    "    '''\n",
    "    results = []\n",
    "    for i in tqdm(range(nb_rnd_test)):\n",
    "        new_params = {}\n",
    "        \n",
    "        # Select a random combinations of parameters\n",
    "        for k,v in parameters_lists[model].items():\n",
    "            new_params[k] = np.random.choice(v)\n",
    "        \n",
    "        param_to_test_with={model:new_params.copy()}\n",
    "        \n",
    "        # Train the model and store the results\n",
    "        best_w, avg_loss = train_model(tX, y, model, initial_w, param_to_test_with, verbose)\n",
    "        results.append((avg_loss, best_w, param_to_test_with))\n",
    "        \n",
    "    # Return the parameters with the smaller loss\n",
    "    return sorted(results, key=lambda e: e[0])[0], nb_rnd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate parameters posibilities (modify these as you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameters (adapt these as you want to) depending on the one we need for the given model\n",
    "rnd_grid_search_params = {\n",
    "    'LR':{\n",
    "        'lambda_': [x for x in np.linspace(0.01,0.21,100)],\n",
    "        'lr': [x for x in np.linspace(0.01,0.21,100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1,10,10)],\n",
    "        'poly':[i for i in range(1,5)]\n",
    "    },\n",
    "    'LS_GD':{\n",
    "        'lr': [x for x in np.linspace(0.01,0.21,100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1,10,10)],\n",
    "        'poly':[i for i in range(1,5)]\n",
    "    },\n",
    "    'LS_SGD':{\n",
    "        'lr': [x for x in np.linspace(0.01,0.21,100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1,10,10)],\n",
    "        'poly':[i for i in range(1,5)],\n",
    "        'batch_size':[1]\n",
    "    },\n",
    "    'LS_normal':{\n",
    "        'max_iters': [1],\n",
    "        'K': [int(i) for i in np.linspace(1,10,10)],\n",
    "        'poly':[i for i in range(1,5)]\n",
    "    },\n",
    "    'RR_normal':{\n",
    "        'max_iters': [1],\n",
    "        'K': [int(i) for i in np.linspace(1,10,10)],\n",
    "        'lambda_': [x for x in np.linspace(0.01,0.21,100)],\n",
    "        'poly':[i for i in range(1,5)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def search_and_save_params(model, tX, y, initial_w, parameters_lists, nb_rnd_test, verbose=False):\n",
    "    result = randomized_grid_search(model,\n",
    "                                    tX,\n",
    "                                    y,\n",
    "                                    initial_w,\n",
    "                                    parameters_lists,\n",
    "                                    nb_rnd_test,\n",
    "                                    verbose=False)\n",
    "    now = datetime.now().strftime('%m(month)-%d_%Hh%Mm%Ss')\n",
    "    model = list(result[0][2].keys())[0]\n",
    "\n",
    "    OUTPUT_PATH = '../data/results/param_tuning_' + model + '_' + str(now) + '.txt'\n",
    "\n",
    "    data=str('avg_loss:{}\\nparameters:{}\\nnb_rnd_try:{}\\nw:{}'.format(\n",
    "        result[0][0], result[0][2], result[1], result[0][1]))\n",
    "\n",
    "    with open(OUTPUT_PATH,'w') as output:\n",
    "        output.write(str(data))\n",
    "    \n",
    "##################################################################\n",
    "# /!\\ If this fail, create a directory named 'results' in 'data' #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_save_params(model='LS_normal',\n",
    "                       tX=tX,\n",
    "                       y=y,\n",
    "                       initial_w=None,\n",
    "                       parameters_lists=rnd_grid_search_params,\n",
    "                       nb_rnd_test=50,\n",
    "                       verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with special parameters: \n",
    "\n",
    "For other parameters than in the default, change json or create a new dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "## Training with k-fold:\n",
    "model = 'LR'\n",
    "\n",
    "with open('parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "best_w, avg_loss = train_model(tX=tX,\n",
    "                               model=model,\n",
    "                               y=y,\n",
    "                               initial_w=None,\n",
    "                               param=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(f'test data shape: X {tX_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard par rapport à moyenne et std de train:\n",
    "POLY = parameters[model]['poly']\n",
    "\n",
    "poly_X_test =  poly_feats(tX_test, POLY)\n",
    "poly_X_train = poly_feats(tX, POLY)\n",
    "\n",
    "mean_train = np.mean(poly_X_train[:,1:], axis=0)\n",
    "std_train = np.std(poly_X_train[:,1:] - mean_train, axis=0)\n",
    "\n",
    "tX_test_std = standardize_with_mean_std(poly_X_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose method: \n",
    "from datetime import date\n",
    "today = date.today().strftime('%m-%d')\n",
    "\n",
    "# parameters:\n",
    "params = ''\n",
    "for param in parameters[model]:\n",
    "    params+= param+'='+str(parameters[model][param])+','\n",
    "\n",
    "test_prediction = predict_labels(best_w, tX_test_std)\n",
    "OUTPUT_PATH = '../data/results/y_pred_' + model + '_'+str(today)+'_'+params+'.csv'\n",
    "ids_test = pd.read_csv('../data/sample-submission.csv')['Id']\n",
    "create_csv_submission(ids_test, test_prediction, OUTPUT_PATH)\n",
    "\n",
    "##################################################################\n",
    "# /!\\ If this fail, create a directory named 'results' in 'data' #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "\n",
    "- Grid search for best parameters\n",
    "- Pre-processing: look for outliers, etc\n",
    "- Dimension reduction by PCA or Laplacian dimension reduction.\n",
    "- Etude comparative\n",
    "- implentations.py\n",
    "- Rapport\n",
    "\n",
    "### Questions: \n",
    "- encoding of y : 0vs1 / -1vs1 ? \n",
    "- predict labels ? \n",
    "Jäggi: more/better data cleaning, preprocessing and feature expansions or variants for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
