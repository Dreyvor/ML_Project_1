{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "# importing the module\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#additional functions:\n",
    "from add_functions.proj1_helpers import *\n",
    "from add_functions.ml_methods_labs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: \n",
    "\n",
    "- train.csv - Training set of 250000 events. The file starts with the ID column, then the label column (the y you have to predict), and finally 30 feature columns.\n",
    "- test.csv - The test set of around 568238 events - Everything as above, except the label is missing.\n",
    "- sample-submission.csv - a sample submission file in the correct format. The sample submission always predicts -1, that is ‘background’.\n",
    "\n",
    "Note: \n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training data shape: y {y.shape}, X {tX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### standardize data:\n",
    "tX_std = standardize(tX)\n",
    "\n",
    "#check if standardization method correct:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(tX)\n",
    "#print(scaler.transform(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot a few samples of first two coordinates:\n",
    "NUM_POINTS = 40\n",
    "plt.scatter(tX[:NUM_POINTS, 0], tX[:NUM_POINTS, 1], c=y[:NUM_POINTS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ML methods:\n",
    "\n",
    "You should take care of the following:\n",
    "- Return type: Note that all functions should return: (w, loss), which is the last weight vector of the method, and the corresponding loss value (cost function). Note that while in previous labs you might have kept track of all encountered w for iterative methods, here we only want the last one.\n",
    "- File names: Please provide all function implementations in a single python file, called implementations.py.\n",
    "- All code should be easily readable and commented.\n",
    "- Note that we might automatically call your provided methods and evaluate for correct implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation:\n",
    "Create cross validation method in order to evaluate the loss better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_sets(tX, y, percentage):\n",
    "    \"\"\"\n",
    "    cross_validation_sets: separates tX and y randomly into training and validation sets. \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - double percentage: percentage of training from original set (e.g. 0.8)\n",
    "    @output: \n",
    "    - np.array(percentage*N,m) tX_train: training features\n",
    "    - np.array(percentage*N,) y_train: training labels\n",
    "    - np.array((1-percentage)*N,m) tX_val: validation features\n",
    "    - np.array((1-percentage)**N,) y_val: validation labels\n",
    "    \"\"\"\n",
    "    tX = pd.DataFrame(tX)\n",
    "    y = pd.DataFrame(y)\n",
    "    indices = tX.index.values\n",
    "\n",
    "    #select a percentage of random rows random rows\n",
    "    train_indices = np.random.choice(range(0, indices[-1]),\n",
    "                                     size=int(indices[-1] * percentage),\n",
    "                                     replace=False)\n",
    "    #creates training and validation:\n",
    "    tX_train = tX.iloc[train_indices].values\n",
    "    tX_val = tX.drop(index=train_indices, axis=0).values\n",
    "    y_train = y.iloc[train_indices].values[:, 0]\n",
    "    y_val = y.drop(index=train_indices, axis=0).values[:, 0]\n",
    "\n",
    "    assert (tX_train.shape[0] + tX_val.shape[0] == len(tX.index.values))\n",
    "    assert (y_train.shape[0] + y_val.shape[0] == len(tX.index.values))\n",
    "\n",
    "    return tX_train, y_train, tX_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models that will be trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LS_GD': 'Least squares with GD',\n",
    "    'LS_SGD': 'Least squares with SGD',\n",
    "    'LS_normal': 'Least squares with normal equations',\n",
    "    'RR_normal': 'Ridge regression with normal equations',\n",
    "    'LR': 'Logistic regression with GD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default json parameters for training, for special parameters take other file or create your own dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: \n",
    "Global training function where each model can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(tX,\n",
    "                y,\n",
    "                model='LS_GD',\n",
    "                initial_w=np.zeros(tX.shape[1]),\n",
    "                param=default_parameters):\n",
    "    \"\"\"\n",
    "    train_model: trains a model according to their parameters in the dictionnary\n",
    "    @input:\n",
    "        - np.array(N,) y: labels\n",
    "        - np.array(N,m) tX: features\n",
    "        - string model: 'LS_GD', 'LS_SGD', 'LS_normal', 'RR_normal' or 'LR'\n",
    "        - np.array(m,) initial_w: starting weights for GD and SGD\n",
    "        - dictionary param: different parameters required for training (lr, lambda, etc)\n",
    "    @output: \n",
    "        - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "        - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    # will keep last weights and last loss\n",
    "    weights, loss = [], []\n",
    "    parameters = param[model]\n",
    "    # percentage of cross-validation sets\n",
    "    PERCENTAGE = parameters['perc']\n",
    "    # number of cross-validation\n",
    "    K = parameters['K']\n",
    "    # max iterations of GD\n",
    "    MAX_ITERS = parameters['max_iters']\n",
    "    # will keep all losses during training\n",
    "    costs_ = np.zeros([K, MAX_ITERS])\n",
    "\n",
    "    print(\n",
    "        f'{models[model]} starting with: \\nInitial_w: {initial_w},\\nParameters:{parameters}'\n",
    "    )\n",
    "    print('------------------')\n",
    "    print('START TRAINING:')\n",
    "\n",
    "    for i in range(K):\n",
    "        print(f'K = {i+1}')\n",
    "        # create training and validation sets:\n",
    "        tX_train, y_train, tX_val, y_val = cross_validation_sets(\n",
    "            tX, y, PERCENTAGE)\n",
    "\n",
    "        #start with initial_w:\n",
    "        w = initial_w.copy()\n",
    "        # keep tabs on losses during training\n",
    "        cost_history = []\n",
    "\n",
    "        # if RR_normal or LS_normal no GD:\n",
    "        if model == 'RR_normal':\n",
    "            w, cost_history = ridge_regression_update(tX_train, y_train,\n",
    "                                                      tX_val, y_val,\n",
    "                                                      parameters)\n",
    "            cost = cost_history\n",
    "            print(f'Final loss:{cost}')\n",
    "        elif model == 'LS_normal':\n",
    "            w, cost_history = least_squares_update(tX_train, y_train, tX_val,\n",
    "                                                   y_val)\n",
    "            cost = cost_history\n",
    "            print(f'Final loss:{cost}')\n",
    "        # else GD:\n",
    "        else:\n",
    "            for j in range(MAX_ITERS):\n",
    "                if model == 'LR':\n",
    "                    # calculate loss:\n",
    "                    cost = cost_logistic(tX_val, y_val, w, parameters)\n",
    "                    cost_history.append(cost)\n",
    "                    # Update weights:\n",
    "                    w = update_weights_logistic(tX_train, y_train, w,\n",
    "                                                parameters)\n",
    "                if model == 'LS_GD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(y_val, tX_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    # Update weights:\n",
    "                    w = update_weights_LS_GD(tX_train, y_train, w, parameters)\n",
    "                if model == 'LS_SGD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(y_val, tX_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    # update weights:\n",
    "                    w = update_weights_LS_SGD(tX_train, y_train, w, parameters)\n",
    "\n",
    "            print(f'Final loss:{cost}')\n",
    "        # Add last weights and loss for auditing:\n",
    "        weights.append(w)\n",
    "        loss.append(cost)\n",
    "        costs_[i] = cost_history\n",
    "        print('------------------')\n",
    "    print('Average loss: {avg_loss}'.format(avg_loss=np.mean(loss)))\n",
    "\n",
    "    # best weights of min loss:\n",
    "    best_w = weights[np.argmin(loss)]\n",
    "\n",
    "    # Plot loss evolution for GD:\n",
    "    if model != 'LS_normal' and model != 'RR_normal':\n",
    "        print('------------------')\n",
    "        print('Loss evolution:')\n",
    "        plt.plot(costs_.T)\n",
    "        plt.title(models[model])\n",
    "        plt.legend(range(1, K + 1))\n",
    "\n",
    "    return best_w, np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares GD: \n",
    "Linear regression using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_MSE(y, tx, w):\n",
    "    \"\"\"\n",
    "    gradient_MSE: calculates the gradient of the MSE function \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: np.array(m,) gradient of MSE\n",
    "    \"\"\"\n",
    "    return (-1 / len(y)) * tx.T @ (y - tx @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(y, tx, w):\n",
    "    \"\"\"\n",
    "    MSE_loss: calculates the MSE loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: double, MSE loss\n",
    "    \"\"\"\n",
    "    MSE = np.square(np.subtract(y,tx @ w)).mean()\n",
    "    return MSE\n",
    "\n",
    "# Test MSE loss function with random weight vector:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "w = np.random.rand(tX_std.shape[1])\n",
    "assert (mean_squared_error(y, tX_std @ w) == MSE_loss(y, tX_std, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_GD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_GD: one step of GD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to GD\n",
    "    \"\"\"    \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # calculate gradient:\n",
    "    grad = gradient_MSE(y, tX, w)\n",
    "    # update weights:\n",
    "    w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(tx, y, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_GD: training with least squares GD\n",
    "    @input:\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LS_GD'\n",
    "    best_w, avg_loss = train_model(tx, y, model, initial_w, parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Test of LS-GD:\n",
    "initial_w = np.zeros(tX_std.shape[1])\n",
    "best_w, avg_loss = least_squares_GD(tX_std, y, initial_w, default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares SGD:\n",
    "Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_SGD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_SGD: one step of SGD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to SGD\n",
    "    \"\"\"   \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # batch_size to 1 for SGD, otherwise mini-batch:\n",
    "    batch_size = parameters['batch_size']\n",
    "\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tX, batch_size):\n",
    "        # calculate gradient on batch:\n",
    "        grad = gradient_MSE(minibatch_y, minibatch_tx, w)\n",
    "        # update weights:\n",
    "        w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(tx, y,initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_SGD: training with least squares SGD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LS_SGD'\n",
    "    best_w, avg_loss = train_model(tx, y, model, initial_w, parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS-SGD:\n",
    "initial_w = np.zeros(tX_std.shape[1])\n",
    "best_w, avg_loss = least_squares_SGD(tX_std, y, initial_w, default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares:\n",
    "Least squares regression using normal equations. Solve $A^TAx = A^Ty$, Ax = b → x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_update(tX_train, y_train, tX_val, y_val):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    @output: \n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"        \n",
    "    # \"train\": \n",
    "    w = np.linalg.solve(tX_train.T @ tX_train, tX_train.T @ y_train)\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(y_val, tX_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas nécessaire de garder cette fonction `least_squares` si c'est ok d'avoir juste une fx train, un peu inutile... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx, parameters):\n",
    "    \"\"\"\n",
    "    least_squares: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LS_normal'\n",
    "    best_w, avg_loss = train_model(tx, y, model, initial_w, parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS_normal:\n",
    "best_w, avg_loss = least_squares(y, tX_std, default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression: \n",
    "Ridge regression using normal equations. $w^* = (X^T X+\\lambda I)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_update(tX_train, y_train, tX_val, y_val, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output:\n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"\n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "\n",
    "    # \"train\":\n",
    "    w = np.linalg.solve(\n",
    "        tX_train.T @ tX_train + lambda_ * np.eye(tX_train.shape[1]),\n",
    "        tX_train.T @ y_train)\n",
    "\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(y_val, tX_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nouveau, pas nécessaire de garder cette fonction `ridge_regression` si c'est ok d'avoir juste une fx train, un peu inutile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, parameters):\n",
    "    \"\"\"\n",
    "    ridge_regression: weights with normal equations of ridge regression\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'RR_normal'\n",
    "    best_w, avg_loss = train_model(tx, y, model, initial_w, parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of RR_normal:\n",
    "best_w, avg_loss = ridge_regression(y, tX_std,default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (regularized) regression: \n",
    "Logistic regression using gradient descent or SGD. \n",
    "https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb\n",
    "\n",
    "https://machinelearningmedium.com/2017/09/15/regularized-logistic-regression/\n",
    "\n",
    "- cost : $J(w) = -1/m (-y^T log (h)-(1-y)^T log (1-h)) + \\lambda/(2m)||w||^2 ; h = sigmoid(Xw)$\n",
    "\n",
    "Regularized logistic regression using gradient descent or SGD. $ w^* = min_w \\sum_{i=1}^n log(1+exp(-y_i w^T x_i)) +\\lambda ‖w‖^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    \"\"\"\n",
    "    sigmoid_activation: calculates the sigmoid activation of a vector z\n",
    "    @output: np.array(m,) \n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    cost_logistic: calculates the logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - double cost: logistic loss\n",
    "    \"\"\"     \n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "    \n",
    "    predictions = sigmoid_activation(tX @ w)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #Take the sum of both costs: error when label=1 + error when label=0\n",
    "    cost = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "    \n",
    "    #Take the average cost:\n",
    "    cost = cost.sum() / m\n",
    "\n",
    "    # regularizer:\n",
    "    if lambda_:\n",
    "        cost += (lambda_ / (2 * m)) * w.T @ w\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_logistic: one iteration with GD on logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights to be updated\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) w: new weights\n",
    "    \"\"\"     \n",
    "    \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    lambda_ = parameters['lambda_']\n",
    "    m = len(tX)\n",
    "    \n",
    "    #Get Predictions:\n",
    "    predictions = sigmoid_activation(np.dot(tX, w))\n",
    "    \n",
    "    # slope of the cost function across all observations\n",
    "    gradient = (tX.T @ (predictions - y))\n",
    "\n",
    "    # if regularization\n",
    "    if lambda_:\n",
    "        gradient += lambda_ * w\n",
    "    \n",
    "    gradient /= m\n",
    "    \n",
    "    # Subtract from our weights to minimize cost\n",
    "    w -= lr * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    logistic_regression: logistic (regularized) regression with GD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'LR'\n",
    "    best_w, avg_loss = train_model(tx, y, model, initial_w, parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of logistic regression:\n",
    "# set lambda to None if no regularization:\n",
    "parameters['lambda_'] = None\n",
    "initial_w = np.zeros(tX_std.shape[1])\n",
    "best_w, avg_loss = logistic_regression(y, tX_std,initial_w, default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of logistic regularized regression:\n",
    "initial_w = np.zeros(tX_std.shape[1])\n",
    "best_w, avg_loss = logistic_regression(y, tX_std,initial_w, default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with special parameters: \n",
    "\n",
    "For other parameters than in the default, change json or create a new dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "## Training with k-fold:\n",
    "model = 'LR'\n",
    "initial_w = np.zeros(tX_std.shape[1])\n",
    "\n",
    "with open('parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "best_w, avg_loss = train_model(tX_std, y, model, initial_w, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(f'test data shape: X {tX_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard par rapport à moyenne et std de train:\n",
    "mean_train = np.mean(tX, axis=0)\n",
    "std_train = np.std(tX - mean_train, axis=0)\n",
    "test_data = standardize_with_meanstd(tX_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose method: \n",
    "from datetime import date\n",
    "today = date.today().strftime('%m-%d')\n",
    "\n",
    "# parameters:\n",
    "params = ''\n",
    "for param in parameters[model]:\n",
    "    params+= param+'='+str(parameters[model][param])+','\n",
    "\n",
    "test_prediction = predict_labels(best_w, test_data)\n",
    "OUTPUT_PATH = '../data/results/y_pred_' + model + '_'+str(today)+'_'+params+'.csv'\n",
    "ids_test = pd.read_csv('../data/sample-submission.csv')['Id']\n",
    "create_csv_submission(ids_test, test_prediction, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "\n",
    "- Grid search for best parameters\n",
    "- Pre-processing: look for outliers, etc\n",
    "- Dimension reduction by PCA or Laplacian dimension reduction.\n",
    "- Etude comparative\n",
    "- implentations.py\n",
    "- Rapport\n",
    "\n",
    "Jäggi: more/better data cleaning, preprocessing and feature expansions or variants for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
