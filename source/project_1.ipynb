{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "# importing the module\n",
    "import json\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#additional functions:\n",
    "from add_functions.proj1_helpers import *\n",
    "from add_functions.ml_methods_labs import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: \n",
    "\n",
    "- train.csv - Training set of 250000 events. The file starts with the ID column, then the label column (the y you have to predict), and finally 30 feature columns.\n",
    "- test.csv - The test set of around 568238 events - Everything as above, except the label is missing.\n",
    "- sample-submission.csv - a sample submission file in the correct format. The sample submission always predicts -1, that is ‘background’.\n",
    "\n",
    "**Note:**\n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(f'training data shape: y {y.shape}, X {tX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ind = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis',\n",
    "       'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
    "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
    "       'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
    "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi',\n",
    "       'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
    "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
    "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
    "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20, 20))\n",
    "m = 0\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        bins = np.linspace(np.min(tX[:, m]), np.max(tX[:, m]), 50)\n",
    "        axs[j, i].hist(tX[:, m], bins = bins)\n",
    "        axs[j, i].set_title(data_ind[m])\n",
    "        m += 1\n",
    "plt.suptitle('Histograms of raw data:', fontsize=14)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invalid values:\n",
    "Histogram after replacing -999 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_invalid_values(tX, invalid_identifier, mean=True):\n",
    "    \"\"\"Replaces invalid values with the mean of all the values in the cooresponding feature \"\"\"\n",
    "    means = []\n",
    "    data = tX.copy()\n",
    "    new_data = np.empty((tX.shape[0], 1))\n",
    "\n",
    "    for i in range(0, tX.shape[1]):\n",
    "        # deleting all occurences of invalid_identifier:\n",
    "        column = data[:, i].copy()\n",
    "        column = np.delete(column, np.where(column == invalid_identifier))\n",
    "        if mean:\n",
    "            # replacing all the invalid data with the above\n",
    "            data[:, i] = np.where(data[:, i] == invalid_identifier,\n",
    "                                  np.mean(column), data[:, i])\n",
    "            new_data = np.concatenate(\n",
    "                (new_data, np.reshape(data[:, i], (tX.shape[0], 1))), axis=1)\n",
    "            means.append(np.mean(column))\n",
    "        else:\n",
    "            data[:, i] = np.where(data[:, i] == invalid_identifier,\n",
    "                                  np.median(column), data[:, i])\n",
    "            new_data = np.concatenate(\n",
    "                (new_data, np.reshape(data[:, i], (tX.shape[0], 1))), axis=1)\n",
    "            means.append(np.median(column))\n",
    "    assert(new_data[:, 1:].shape == tX.shape)\n",
    "    return new_data[:, 1:], means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_identifier = -999\n",
    "tX_invalid, medians = replace_invalid_values(tX,\n",
    "                                              invalid_identifier,\n",
    "                                              mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20, 20))\n",
    "m = 0\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        bins = np.linspace(np.min(tX_invalid[:, m]), np.max(tX_invalid[:, m]), 50)\n",
    "        axs[j, i].hist(tX_invalid[:, m], bins=bins, label = 'data without invalid numbers')\n",
    "        axs[j, i].set_title(data_ind[m])\n",
    "        axs[j, i].axvline(medians[m], color='r', label = 'median')\n",
    "        m += 1\n",
    "plt.legend()\n",
    "plt.suptitle('Histograms of filtered data:', fontsize=14)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outlayers removing\n",
    "# we consider a value as outlay when x > delta*std_deviationtX = replace_outlayers_values_with_mean(tX, delta)\n",
    "def replace_outlayers_values(features, delta, mean=True):\n",
    "    \"\"\"Replaces outlayers values with the mean of all the values in the cooresponding feature \"\"\"\n",
    "    medians = []\n",
    "    new_data = np.empty((len(features), 1))\n",
    "    for i in range(30):\n",
    "        column = features[:, i].copy()\n",
    "        Q1 = np.quantile(column, 0.10)\n",
    "        Q3 = np.quantile(column, 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        if mean:\n",
    "            mean = np.mean(column)\n",
    "            col_no_outl = np.where(\n",
    "                ~((column < (Q1 - delta * IQR)) | (column > (Q3 + delta * IQR))),\n",
    "                column, mean)\n",
    "            new_data = np.concatenate(\n",
    "                (new_data, np.reshape(col_no_outl, (len(features), 1))), axis=1)\n",
    "            medians.append(mean)\n",
    "        else:\n",
    "            median = np.median(column)\n",
    "            col_no_outl = np.where(\n",
    "                ~((column < (Q1 - delta * IQR)) | (column > (Q3 + delta * IQR))),\n",
    "                column, median)\n",
    "            new_data = np.concatenate(\n",
    "                (new_data, np.reshape(col_no_outl, (len(features), 1))), axis=1)\n",
    "            medians.append(median)\n",
    "    return new_data[:,1:], medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.5\n",
    "tX_filtered, medians = replace_outlayers_values(tX_invalid, delta, mean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_filtered == tX_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(20, 20))\n",
    "m = 0\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        bins1 = np.linspace(np.min(tX_filtered[:, m]),\n",
    "                           np.max(tX_filtered[:, m]), 20)\n",
    "        bins2 = np.linspace(np.min(tX_invalid[:, m]),\n",
    "                           np.max(tX_invalid[:, m]), 20)\n",
    "        \n",
    "        axs[j, i].hist(tX_invalid[:, m],\n",
    "                       color='orange',\n",
    "                       bins=bins2,\n",
    "                       alpha=0.5,\n",
    "                       label='invalid')\n",
    "        axs[j, i].hist(tX_filtered[:, m],\n",
    "                       color='blue',\n",
    "                       bins=bins1,\n",
    "                       alpha=0.5,\n",
    "                       label='outliers')\n",
    "        axs[j, i].set_title(data_ind[m])\n",
    "        axs[j, i].axvline(medians[m], color='r')\n",
    "        m += 1\n",
    "plt.legend()\n",
    "plt.suptitle('Histograms of filtered data:', fontsize=14)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding of y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding of y: \n",
    "y_enc = (y+1)/2\n",
    "y = y_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_invalid    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ML methods:\n",
    "\n",
    "You should take care of the following:\n",
    "- Return type: Note that all functions should return: (w, loss), which is the last weight vector of the method, and the corresponding loss value (cost function). Note that while in previous labs you might have kept track of all encountered w for iterative methods, here we only want the last one.\n",
    "- File names: Please provide all function implementations in a single python file, called implementations.py.\n",
    "- All code should be easily readable and commented.\n",
    "- Note that we might automatically call your provided methods and evaluate for correct implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation:\n",
    "Create cross validation method in order to evaluate the loss better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed=2):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_sets(tX, y, k_indices,i):\n",
    "    \"\"\"\n",
    "    cross_validation_sets: separates tX and y randomly into training and validation sets. \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    - int K: number of folds during k-fold cross-val\n",
    "    @output: \n",
    "    - np.array(percentage*N,m) tX_train: training features\n",
    "    - np.array(percentage*N,) y_train: training labels\n",
    "    - np.array((1-percentage)*N,m) tX_val: validation features\n",
    "    - np.array((1-percentage)**N,) y_val: validation labels\n",
    "    \"\"\"\n",
    "    \n",
    "    train_indices = np.concatenate(np.delete(k_indices, i, axis = 0), axis = 0)\n",
    "    val_indices = k_indices[i]\n",
    "    \n",
    "    #creates training and validation:\n",
    "    tX_train = np.take(tX, train_indices, axis = 0)\n",
    "    y_train = np.take(y, train_indices, axis = 0)\n",
    "    tX_val = np.take(tX, val_indices, axis = 0)\n",
    "    y_val = np.take(y, val_indices, axis = 0)\n",
    "    \n",
    "    size = len(train_indices) + len(val_indices)\n",
    "    \n",
    "    assert (tX_train.shape[0] + tX_val.shape[0] == size)\n",
    "    assert (y_train.shape[0] + y_val.shape[0] == size)\n",
    "\n",
    "    return tX_train, y_train, tX_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models that will be trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LS_GD': 'Least squares with GD',\n",
    "    'LS_SGD': 'Least squares with SGD',\n",
    "    'LS_normal': 'Least squares with normal equations',\n",
    "    'RR_normal': 'Ridge regression with normal equations',\n",
    "    'LR': 'Logistic regression with GD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default json parameters for training, for special parameters take other file or create your own dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Dimension reduction:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tX_std = standardize(tX)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(tX_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "#tX_pc = pd.DataFrame(pca.fit_transform(tX_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc = pd.DataFrame(pca.fit_transform(tX_std))\n",
    "#tX_pc['label'] = y\n",
    "#tX_pc['index'] = tX_pc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc['label'] = y\n",
    "tX_pc['index'] = tX_pc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pc_sample = tX_pc.iloc[np.random.choice(len(tX_std),200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tX_pc_sample[0],tX_pc_sample[1], c=tX_pc_sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#sns.pairplot(tX_pc_sample, hue = 'label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion:\n",
    "Polynomial feature expansion: $X' = [1, X, X^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feats(tX, degree):  \n",
    "    if not np.array_equal(tX[:, 0], np.ones(len(tX))):\n",
    "            # add bias term:\n",
    "            tX_poly = np.hstack((np.ones((len(tX), 1)), tX))\n",
    "    if degree>1: \n",
    "        for deg in range(2, degree+1):\n",
    "            tX_poly = np.c_[tX_poly, np.power(tX, deg)]\n",
    "    return tX_poly\n",
    "\n",
    "assert (np.array_equal(\n",
    "    poly_feats(tX, 2)[:, 1:31],\n",
    "    tX[:, 0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy:\n",
    "def accuracy(feautres, w, true_y):\n",
    "    y_pred = predict_labels(w, feautres)\n",
    "    # encode to 0/1\n",
    "    y_pred_enc = (y_pred+1)/2\n",
    "    P_N = len(y_pred_enc[np.where(np.subtract(y_pred_enc, true_y)==0)])\n",
    "    #return sklearn.metrics.accuracy_score(y_pred_enc, true_y), (P_N/len(y_val))\n",
    "    return (P_N/len(true_y))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: \n",
    "Global training function where each model can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(tX,\n",
    "                y,\n",
    "                model='LS_GD',\n",
    "                initial_w=None,\n",
    "                param=default_parameters,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "    train_model: trains a model according to their parameters in the dictionnary\n",
    "    @input:\n",
    "        - np.array(N,) y: labels\n",
    "        - np.array(N,m) tX: features\n",
    "        - string model: 'LS_GD', 'LS_SGD', 'LS_normal', 'RR_normal' or 'LR'\n",
    "        - np.array(m,) initial_w: starting weights for GD and SGD\n",
    "        - dictionary param: different parameters required for training (lr, lambda, etc)\n",
    "    @output: \n",
    "        - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "        - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    # will keep last weights and last loss\n",
    "    weights, loss = [], []\n",
    "    parameters = param[model]\n",
    "\n",
    "    # parameters:\n",
    "    K, MAX_ITERS, POLY = parameters['K'], parameters['max_iters'], parameters[\n",
    "        'poly']\n",
    "\n",
    "    # will keep all losses during training\n",
    "    costs_ = np.zeros([K, MAX_ITERS])\n",
    "    train_costs_ = np.zeros([K, MAX_ITERS])\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f'{models[model]} starting with: \\nInitial_w: {initial_w},\\nParameters:{parameters}'\n",
    "        )\n",
    "        print('------------------')\n",
    "        print('START TRAINING:')\n",
    "\n",
    "    # get indices of k-fold:\n",
    "    k_indices = build_k_indices(y, K)\n",
    "\n",
    "    # polynonmial expansion:\n",
    "    tX_pol = poly_feats(tX, POLY)\n",
    "\n",
    "    # standardize:\n",
    "    tX_std = standardize(tX_pol)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Data shape:{tX_std.shape}')\n",
    "\n",
    "    # initial_w:\n",
    "    if initial_w == None:\n",
    "        initial_w = np.zeros(tX_std.shape[1])\n",
    "\n",
    "    for i in range(K):\n",
    "        if verbose:\n",
    "            print(f'K = {i+1}')\n",
    "        # create training and validation sets:\n",
    "        tX_train, y_train, tX_val, y_val = cross_validation_sets(\n",
    "            tX_std, y, k_indices, i)\n",
    "\n",
    "        #start with initial_w:\n",
    "        w = initial_w.copy()\n",
    "        # keep tabs on losses during training\n",
    "        cost_history = []\n",
    "        train_cost_history = []\n",
    "\n",
    "        # if RR_normal or LS_normal no GD:\n",
    "        if model == 'RR_normal':\n",
    "            w, cost_history = ridge_regression_update(tX_train, y_train,\n",
    "                                                      tX_val, y_val,\n",
    "                                                      parameters)\n",
    "            cost = cost_history\n",
    "            acc = accuracy(tX_val, w, y_val)\n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "                print(f'Final accuracy:{acc}')\n",
    "\n",
    "        elif model == 'LS_normal':\n",
    "            w, cost_history = least_squares_update(tX_train, y_train, tX_val,\n",
    "                                                   y_val)\n",
    "            acc = accuracy(tX_val, w, y_val)\n",
    "            cost = cost_history\n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "                print(f'Final accuracy:{acc}')\n",
    "\n",
    "        # else GD:\n",
    "        else:\n",
    "            for j in range(MAX_ITERS):\n",
    "                if model == 'LR':\n",
    "                    # calculate loss:\n",
    "                    cost = cost_logistic(tX_val, y_val, w, parameters)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(\n",
    "                        cost_logistic(tX_train, y_train, w, parameters))\n",
    "                    # Update weights:\n",
    "                    w = update_weights_logistic(tX_train, y_train, w,\n",
    "                                                parameters)\n",
    "                    acc = accuracy(tX_val, w, y_val)\n",
    "                if model == 'LS_GD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(tX_val, y_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(MSE_loss(tX_train, y_train, w))\n",
    "                    # Update weights:\n",
    "                    w = update_weights_LS_GD(tX_train, y_train, w, parameters)\n",
    "                    acc = accuracy(tX_val, w, y_val)\n",
    "                if model == 'LS_SGD':\n",
    "                    # calulate loss:\n",
    "                    cost = MSE_loss(tX_val, y_val, w)\n",
    "                    cost_history.append(cost)\n",
    "                    train_cost_history.append(MSE_loss(tX_train, y_train, w))\n",
    "                    # update weights:\n",
    "                    w = update_weights_LS_SGD(tX_train, y_train, w, parameters)\n",
    "                    acc = accuracy(tX_val, w, y_val)\n",
    "            if verbose:\n",
    "                print(f'Final loss:{cost}')\n",
    "                print(f'Final accuracy:{acc}')\n",
    "        # Add last weights and loss for auditing:\n",
    "        weights.append(w)\n",
    "        loss.append(cost)\n",
    "        costs_[i] = cost_history\n",
    "        if model != 'LS_normal' and model != 'RR_normal':\n",
    "            train_costs_[i] = train_cost_history\n",
    "        if verbose:\n",
    "            print('------------------')\n",
    "\n",
    "    if verbose:\n",
    "        print('Average loss: {avg_loss}'.format(avg_loss=np.mean(loss)))\n",
    "\n",
    "    # best weights of min loss:\n",
    "    best_w = weights[np.argmin(loss)]\n",
    "\n",
    "    # Plot loss evolution for GD:\n",
    "    if model != 'LS_normal' and model != 'RR_normal' and verbose:\n",
    "        print('------------------')\n",
    "        print('Loss evolution:')\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].plot(costs_.T)\n",
    "        ax[0].set_title(models[model] + ' , validation loss')\n",
    "        ax[0].legend(range(1, K + 1))\n",
    "        ax[1].plot(train_costs_.T)\n",
    "        ax[1].set_title(models[model] + ' , training loss')\n",
    "        ax[1].legend(range(1, K + 1))\n",
    "\n",
    "    return best_w, np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares GD: \n",
    "Linear regression using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_MSE(tX, y, w):\n",
    "    \"\"\"\n",
    "    gradient_MSE: calculates the gradient of the MSE function \n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: np.array(m,) gradient of MSE\n",
    "    \"\"\"\n",
    "    return (-1 / len(y)) * tX.T @ (y - tX @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE_loss(tX, y, w):\n",
    "    \"\"\"\n",
    "    MSE_loss: calculates the MSE loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    @output: double, MSE loss\n",
    "    \"\"\"\n",
    "    assert(w.shape[0] == tX.shape[1])\n",
    "    assert(y.shape[0] == tX.shape[0])\n",
    "    MSE = np.square(np.subtract(y,tX @ w)).mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_GD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_GD: one step of GD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to GD\n",
    "    \"\"\"\n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # calculate gradient:\n",
    "    grad = gradient_MSE(tX, y, w)\n",
    "    # update weights:\n",
    "    w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(tX, y, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_GD: training with least squares GD\n",
    "    @input:\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    model = 'LS_GD'\n",
    "    best_w, avg_loss = train_model(tX=tX,\n",
    "                                   y=y,\n",
    "                                   model=model,\n",
    "                                   initial_w=initial_w,\n",
    "                                   param=parameters)\n",
    "\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# polynomial feature expansion:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "\n",
    "best_w, avg_loss = least_squares_GD(tX=tX,\n",
    "                                    y=y,\n",
    "                                    initial_w=None,\n",
    "                                    parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares SGD:\n",
    "Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_LS_SGD(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_LS_SGD: one step of SGD with MSE\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: updated weights according to SGD\n",
    "    \"\"\"   \n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    # batch_size to 1 for SGD, otherwise mini-batch:\n",
    "    batch_size = parameters['batch_size']\n",
    "\n",
    "    for minibatch_y, minibatch_tX in batch_iter(y, tX, batch_size):\n",
    "        # calculate gradient on batch:\n",
    "        grad = gradient_MSE(minibatch_tX, minibatch_y, w)\n",
    "        # update weights:\n",
    "        w -= lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(tX, y, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_SGD: training with least squares SGD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    model = 'LS_SGD'\n",
    "    best_w, avg_loss = train_model(tX=tX,\n",
    "                                   y=y,\n",
    "                                   model=model,\n",
    "                                   initial_w=initial_w,\n",
    "                                   param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS-SGD:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = least_squares_SGD(tX=tX,\n",
    "                                     y=y,\n",
    "                                     initial_w=None,\n",
    "                                     parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares:\n",
    "Least squares regression using normal equations. Solve $A^TAx = A^Ty$, Ax = b → x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_update(tX_train, y_train, tX_val, y_val):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    @output: \n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"\n",
    "    # \"train\":\n",
    "    w = np.linalg.solve(tX_train.T @ tX_train, tX_train.T @ y_train)\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(tX_val,y_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas nécessaire de garder cette fonction `least_squares` si c'est ok d'avoir juste une fx train, un peu inutile... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tX, parameters):\n",
    "    \"\"\"\n",
    "    least_squares: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    model = 'LS_normal'\n",
    "    best_w, avg_loss = train_model(tX=tX, y=y, model=model, param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of LS_normal:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = least_squares(y=y, tX=tX, parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression: \n",
    "Ridge regression using normal equations. $w^* = (X^T X+\\lambda I)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_update(tX_train, y_train, tX_val, y_val, parameters):\n",
    "    \"\"\"\n",
    "    least_squares_update: weights with normal equations of least squares\n",
    "    @input:\n",
    "    - np.array(N,) y_train and y_val: training and validation labels\n",
    "    - np.array(N,m) tX_train and tX_val: training and validation features\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output:\n",
    "    - np.array(m,) w: weights \n",
    "    - double loss: MSE loss\n",
    "    \"\"\"\n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "\n",
    "    # \"train\":\n",
    "    w = np.linalg.solve(\n",
    "        tX_train.T @ tX_train + lambda_ * np.eye(tX_train.shape[1]),\n",
    "        tX_train.T @ y_train)\n",
    "\n",
    "    # evaluate loss on validation set:\n",
    "    loss = MSE_loss(tX_val, y_val, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nouveau, pas nécessaire de garder cette fonction `ridge_regression` si c'est ok d'avoir juste une fx train, un peu inutile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tX, parameters):\n",
    "    \"\"\"\n",
    "    ridge_regression: weights with normal equations of ridge regression\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tx: features\n",
    "    @output:\n",
    "    - np.array(m,) best_w: weights with smallest loss during cross-val \n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"     \n",
    "    model = 'RR_normal'\n",
    "    best_w, avg_loss = train_model(tX = tX, y= y, \n",
    "                                   model=model,\n",
    "                                   param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of RR_normal:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = ridge_regression(y = y, tX = tX, parameters = default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (regularized) regression: \n",
    "Logistic regression using gradient descent or SGD. \n",
    "https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb\n",
    "\n",
    "https://machinelearningmedium.com/2017/09/15/regularized-logistic-regression/\n",
    "\n",
    "- cost : $J(w) = -1/m (-y^T log (h)-(1-y)^T log (1-h)) + \\lambda/(2m)||w||^2 ; h = sigmoid(Xw)$\n",
    "\n",
    "Regularized logistic regression using gradient descent or SGD. $ w^* = min_w \\sum_{i=1}^n log(1+exp(-y_i w^T x_i)) +\\lambda ‖w‖^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    \"\"\"\n",
    "    sigmoid_activation: calculates the sigmoid activation of a vector z\n",
    "    @output: np.array(m,) \n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    cost_logistic: calculates the logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - double cost: logistic loss\n",
    "    \"\"\"\n",
    "    # get parameters:\n",
    "    lambda_ = parameters['lambda_']\n",
    "\n",
    "    predictions = sigmoid_activation(tX @ w)\n",
    "    m = y.shape[0]\n",
    "\n",
    "    #Take the sum of both costs: error when label=1 + error when label=0\n",
    "    cost = -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "\n",
    "    #Take the average cost:\n",
    "    cost = cost.sum() / m\n",
    "\n",
    "    # regularizer:\n",
    "    if lambda_:\n",
    "        cost += (lambda_ / (2 * m)) * w.T @ w\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_logistic(tX, y, w, parameters):\n",
    "    \"\"\"\n",
    "    update_weights_logistic: one iteration with GD on logistic (regularized) loss\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) w: weights to be updated\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) w: new weights\n",
    "    \"\"\"\n",
    "\n",
    "    # get parameters:\n",
    "    lr = parameters['lr']\n",
    "    lambda_ = parameters['lambda_']\n",
    "    m = len(tX)\n",
    "\n",
    "    #Get Predictions:\n",
    "    predictions = sigmoid_activation(np.dot(tX, w))\n",
    "\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = (tX.T @ (predictions - y))\n",
    "\n",
    "    # if regularization\n",
    "    if lambda_:\n",
    "        gradient += lambda_ * w\n",
    "\n",
    "    gradient /= m\n",
    "\n",
    "    # Subtract from our weights to minimize cost\n",
    "    w -= lr * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tX, initial_w, parameters):\n",
    "    \"\"\"\n",
    "    logistic_regression: logistic (regularized) regression with GD\n",
    "    @input:\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(m,) initial_w: starting weights\n",
    "    - dict parameters: dictionnary of required parameters\n",
    "    @output: \n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    \"\"\"\n",
    "    model = 'LR'\n",
    "    best_w, avg_loss = train_model(tX=tX,\n",
    "                                   y=y,\n",
    "                                   model=model,\n",
    "                                   initial_w=initial_w,\n",
    "                                   param=parameters)\n",
    "    return best_w, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of logistic regularized regression:\n",
    "with open('default_parameters.json') as json_file:\n",
    "    default_parameters = json.load(json_file)\n",
    "best_w, avg_loss = logistic_regression(tX=tX,\n",
    "                                    y=y,initial_w=None,\n",
    "                                    parameters=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Grid search\n",
    "[Cross-Validation and Hyperparameter Tuning: How to Optimise your Machine Learning Model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def randomized_grid_search(model,\n",
    "                           tX,\n",
    "                           y,\n",
    "                           initial_w,\n",
    "                           parameters_lists,\n",
    "                           nb_rnd_test,\n",
    "                           verbose=False):\n",
    "    '''\n",
    "    randomized_grid_search: Search the best parameters with the model among\n",
    "    a list of value for each parameter.\n",
    "    @input:\n",
    "    - string model: 'LS_GD', 'LS_SGD', 'LS_normal', 'RR_normal' or 'LR'\n",
    "    - np.array(N,m) tX: features\n",
    "    - np.array(N,) y: labels\n",
    "    - np.array(m,) initial_w: starting weights for GD and SGD\n",
    "    - dictionary parameters_lists: A dictionary of lists of values for the parameters\n",
    "        required for training (lr, lambda, etc)\n",
    "    - int nb_rnd_test: number of random test to do before giving the best results it found.\n",
    "    @output\n",
    "    - double avg_loss: average loss over validation sets during cross-val\n",
    "    - np.array(m,) best_w: weights that got the smallest loss during cross-val\n",
    "    - dictionary new_params: hyper-parameters with smallest generalization error\n",
    "    - int nb_rnd_test: same as the input\n",
    "    '''\n",
    "    results = []\n",
    "    for i in tqdm(range(nb_rnd_test)):\n",
    "        new_params = {}\n",
    "\n",
    "        # Select a random combinations of parameters\n",
    "        for k, v in parameters_lists[model].items():\n",
    "            new_params[k] = np.random.choice(v)\n",
    "\n",
    "        param_to_test_with = {model: new_params.copy()}\n",
    "\n",
    "        # Train the model and store the results\n",
    "        best_w, avg_loss = train_model(tX, y, model, initial_w,\n",
    "                                       param_to_test_with, verbose)\n",
    "        results.append((avg_loss, best_w, param_to_test_with))\n",
    "\n",
    "    # Return the parameters with the smaller loss\n",
    "    return sorted(results, key=lambda e: e[0])[0], nb_rnd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate parameters posibilities (modify these as you want to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameters (adapt these as you want to) depending on the one we need for the given model\n",
    "rnd_grid_search_params = {\n",
    "    'LR': {\n",
    "        'lambda_': [x for x in np.linspace(0.01, 0.21, 100)],\n",
    "        'lr': [x for x in np.linspace(0.01, 0.21, 100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1, 10, 10)],\n",
    "        'poly': [i for i in range(1, 5)]\n",
    "    },\n",
    "    'LS_GD': {\n",
    "        'lr': [x for x in np.linspace(0.01, 0.21, 100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1, 10, 10)],\n",
    "        'poly': [i for i in range(1, 5)]\n",
    "    },\n",
    "    'LS_SGD': {\n",
    "        'lr': [x for x in np.linspace(0.01, 0.21, 100)],\n",
    "        'max_iters': [int(x) for x in np.linspace(10, 200, 20)],\n",
    "        'K': [int(i) for i in np.linspace(1, 10, 10)],\n",
    "        'poly': [i for i in range(1, 5)],\n",
    "        'batch_size': [1]\n",
    "    },\n",
    "    'LS_normal': {\n",
    "        'max_iters': [1],\n",
    "        'K': [int(i) for i in np.linspace(1, 10, 10)],\n",
    "        'poly': [i for i in range(1, 5)]\n",
    "    },\n",
    "    'RR_normal': {\n",
    "        'max_iters': [1],\n",
    "        'K': [int(i) for i in np.linspace(1, 10, 10)],\n",
    "        'lambda_': [x for x in np.linspace(0.01, 0.21, 100)],\n",
    "        'poly': [i for i in range(1, 5)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 25\n",
    "with open('parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "    \n",
    "model = 'LR'\n",
    "results = []\n",
    "for i in tqdm(range(experiments)):\n",
    "    \n",
    "    # sample from a Uniform distribution on a log-scale\n",
    "    # Sample learning rate candidates in the range (0.3 to 0.03)\n",
    "    lr = 3*10**np.random.uniform(-1,-2) \n",
    "    # Sample regularization candidates in the range (0.1 to 0.001)\n",
    "    lambda_ = 10**np.random.uniform(-1,-3) \n",
    "    parameters[model]['lr'] = lr\n",
    "    parameters[model]['lambda_'] = lambda_\n",
    "\n",
    "    best_w, avg_loss = train_model(tX=tX,\n",
    "                               model=model,\n",
    "                               y=y,\n",
    "                               initial_w=None,\n",
    "                               param=parameters, verbose = False)\n",
    "    results.append((avg_loss, best_w, parameters[model]))\n",
    "sorted(results, key=lambda e: e[0])[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def search_and_save_params(model,\n",
    "                           tX,\n",
    "                           y,\n",
    "                           initial_w,\n",
    "                           parameters_lists,\n",
    "                           nb_rnd_test,\n",
    "                           verbose=False):\n",
    "    \n",
    "    result = randomized_grid_search(model,\n",
    "                                    tX,\n",
    "                                    y,\n",
    "                                    initial_w,\n",
    "                                    parameters_lists,\n",
    "                                    nb_rnd_test,\n",
    "                                    verbose=False)\n",
    "    \n",
    "    now = datetime.now().strftime('%m(month)-%d_%Hh%Mm%Ss')\n",
    "    model = list(result[0][2].keys())[0]\n",
    "\n",
    "    OUTPUT_PATH = '../data/results/param_tuning_' + model + '_' + str(\n",
    "        now) + '.txt'\n",
    "\n",
    "    data = str('avg_loss:{}\\nparameters:{}\\nnb_rnd_try:{}\\nw:{}'.format(\n",
    "        result[0][0], result[0][2], result[1], result[0][1]))\n",
    "\n",
    "    with open(OUTPUT_PATH, 'w') as output:\n",
    "        output.write(str(data))\n",
    "\n",
    "##################################################################\n",
    "# /!\\ If this fail, create a directory named 'results' in 'data' #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_save_params(model='LS_normal',\n",
    "                       tX=tX,\n",
    "                       y=y,\n",
    "                       initial_w=None,\n",
    "                       parameters_lists=rnd_grid_search_params,\n",
    "                       nb_rnd_test=50,\n",
    "                       verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with special parameters: \n",
    "\n",
    "For other parameters than in the default, change json or create a new dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Training with k-fold:\n",
    "model = 'LR'\n",
    "\n",
    "with open('parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "best_w, avg_loss = train_model(tX=tX,\n",
    "                               model=model,\n",
    "                               y=y,\n",
    "                               initial_w=None,\n",
    "                               param=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(f'test data shape: X {tX_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess X_Test as tX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_identifier = -999\n",
    "tX_test_invalid, medians = replace_invalid_values(tX_test,\n",
    "                                              invalid_identifier,\n",
    "                                              mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.5\n",
    "tX_test_filtered, medians = replace_outlayers_values(tX_test_invalid,\n",
    "                                                     delta,\n",
    "                                                     mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tX_test_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard par rapport à moyenne et std de train:\n",
    "POLY = parameters[model]['poly']\n",
    "\n",
    "poly_X_test =  poly_feats(tX_test, POLY)\n",
    "poly_X_train = poly_feats(tX, POLY)\n",
    "\n",
    "mean_train = np.mean(poly_X_train[:,1:], axis=0)\n",
    "std_train = np.std(poly_X_train[:,1:] - mean_train, axis=0)\n",
    "\n",
    "tX_test_std = standardize_with_mean_std(poly_X_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose method: \n",
    "from datetime import date\n",
    "today = date.today().strftime('%m-%d')\n",
    "\n",
    "# parameters:\n",
    "params = ''\n",
    "for param in parameters[model]:\n",
    "    params+= param+'='+str(parameters[model][param])+','\n",
    "    \n",
    "if not os.path.exists('../data/results'):\n",
    "    os.makedirs('../data/results')\n",
    "    \n",
    "import pandas as pd\n",
    "test_prediction = predict_labels(best_w, tX_test_std)\n",
    "OUTPUT_PATH = '../data/results/y_pred_' + model + '_'+str(today)+'_'+params+'.csv'\n",
    "ids_test = pd.read_csv('../data/sample-submission.csv')['Id']\n",
    "create_csv_submission(ids_test, test_prediction, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: \n",
    "\n",
    "- Grid search for best parameters\n",
    "- Pre-processing: look for outliers, etc\n",
    "- Dimension reduction by PCA or Laplacian dimension reduction.\n",
    "- Etude comparative\n",
    "- implentations.py\n",
    "- Rapport\n",
    "\n",
    "### Questions: \n",
    "- encoding of y : 0vs1 / -1vs1 ? \n",
    "- predict labels ? \n",
    "Jäggi: more/better data cleaning, preprocessing and feature expansions or variants for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
